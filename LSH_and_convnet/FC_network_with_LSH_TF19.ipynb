{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jupyter nbconvert --to script FC_network_with_LSH.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/anaconda3/envs/TF19/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import make_datasets_modified as make_ds\n",
    "import pickle\n",
    "from random import shuffle\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Dense, Dropout\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "num_channels = 3\n",
    "window_size = 40\n",
    "num_vUnits = num_channels*window_size\n",
    "num_KCs = 15*num_vUnits\n",
    "num_ones = num_KCs*6 #total number of ones in matrix\n",
    "p_one = num_ones/(num_vUnits*num_KCs)\n",
    "p_WTA = 0.05\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### must define p_WTA and num_KCs in make_dataset so that\n",
    "### map_fn=WTA_activations_from_values_and_indices_map_fn()\n",
    "### can be used as a map funtion\n",
    "### also features must be in the order values,indices,one_hot_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = '/home/jonas/HDD/data/unwindowed/Fourier_no_emphasis_WTA_vals_and_inds_15xKCs_RS112_WTA005_shuffled/'\n",
    "\n",
    "keys = ['values','indices','one_hot_label']\n",
    "data_types = ['float32','int32','int32']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "training_files = []\n",
    "validation_files = []\n",
    "for file in os.listdir(load_path):\n",
    "    if 'dataset5' in file:\n",
    "        validation_files.append(load_path+file)\n",
    "    elif 'dataset10' in file:\n",
    "        validation_files.append(load_path+file)\n",
    "    elif 'dataset15' in file:\n",
    "        validation_files.append(load_path+file)\n",
    "    elif 'dataset20' in file:\n",
    "        validation_files.append(load_path+file)\n",
    "    elif 'dataset25' in file:\n",
    "        validation_files.append(load_path+file)\n",
    "    else:\n",
    "        training_files.append(load_path+file)\n",
    "print(len(training_files))\n",
    "print(len(validation_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "load_path = '/home/jonas/HDD/data/unwindowed/Fourier_no_emphasis_WTA_vals_and_inds_15xKCs_RS112_WTA005/'\n",
    "validation_files = []\n",
    "for file in os.listdir(load_path):\n",
    "    if 'dataset5' in file:\n",
    "        validation_files.append(load_path+file)\n",
    "    elif 'dataset10' in file:\n",
    "        validation_files.append(load_path+file)\n",
    "    elif 'dataset15' in file:\n",
    "        validation_files.append(load_path+file)\n",
    "    elif 'dataset20' in file:\n",
    "        validation_files.append(load_path+file)\n",
    "    elif 'dataset25' in file:\n",
    "        validation_files.append(load_path+file)\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "validation_files = sorted(validation_files)\n",
    "print(len(validation_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10000/10000 [==============================] - 50s 5ms/step - loss: 1.3099 - acc: 0.4668 - val_loss: 1.4027 - val_acc: 0.4281\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.40267, saving model to /home/jonas/Documents/LSH/LSH_runs/only_softmax_layer/Fourier_no_window_val_fixed/Model.hdf5\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 1.2868 - acc: 0.4776 - val_loss: 1.3426 - val_acc: 0.4559\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.40267 to 1.34256, saving model to /home/jonas/Documents/LSH/LSH_runs/only_softmax_layer/Fourier_no_window_val_fixed/Model.hdf5\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 1.2835 - acc: 0.4785 - val_loss: 1.3388 - val_acc: 0.4610\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.34256 to 1.33876, saving model to /home/jonas/Documents/LSH/LSH_runs/only_softmax_layer/Fourier_no_window_val_fixed/Model.hdf5\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 1.2874 - acc: 0.4751 - val_loss: 1.3674 - val_acc: 0.4523\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 1.33876\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 49s 5ms/step - loss: 1.2833 - acc: 0.4794 - val_loss: 1.3894 - val_acc: 0.4342\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.33876\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 1.2822 - acc: 0.4788 - val_loss: 1.3575 - val_acc: 0.4490\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.33876\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 1.2866 - acc: 0.4758 - val_loss: 1.3629 - val_acc: 0.4527\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.33876\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 1.2825 - acc: 0.4796 - val_loss: 1.3413 - val_acc: 0.4680\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.33876\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 1.2822 - acc: 0.4788 - val_loss: 1.3965 - val_acc: 0.4335\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.33876\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 46s 5ms/step - loss: 1.2856 - acc: 0.4761 - val_loss: 1.3705 - val_acc: 0.4445\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.33876\n"
     ]
    }
   ],
   "source": [
    "num_labels = 5\n",
    "\n",
    "file_names = tf.placeholder(tf.string, shape=[None])\n",
    "train_dataset = make_ds.dataset_from_TFRecords(training_files,batch_size,keys,data_types,\n",
    "                                               shuffle_buffer=250000,num_cores=8,\n",
    "                                               parallel_reads=len(training_files))\n",
    "train_dataset = train_dataset.map(map_func=make_ds.WTA_activations_from_values_and_indices_map_fn,num_parallel_calls=8)\n",
    "train_dataset = train_dataset.repeat()\n",
    "\n",
    "test_dataset = make_ds.dataset_from_TFRecords(validation_files,batch_size,keys,data_types,\n",
    "                                              shuffle_buffer=0,num_cores=8,\n",
    "                                              parallel_reads=len(validation_files))\n",
    "test_dataset = test_dataset.map(map_func=make_ds.WTA_activations_from_values_and_indices_map_fn,num_parallel_calls=8)\n",
    "test_dataset = test_dataset.repeat()\n",
    "\n",
    "a = Input(shape=(num_KCs,))\n",
    "o = Dense(units=num_labels,activation='softmax')(a)\n",
    "\n",
    "model = Model(inputs=a,outputs=o)\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#Train model:\n",
    "summary_path = '/home/jonas/Documents/LSH/LSH_runs/only_softmax_layer/Fourier_no_window_val_fixed/'\n",
    "callbacks = [ModelCheckpoint(summary_path+'Model.hdf5',save_best_only=True,verbose=1)]\n",
    "history = model.fit(train_dataset,epochs=epochs,steps_per_epoch=10000,\n",
    "                    validation_data=test_dataset,validation_steps=1500,\n",
    "                    verbose=1,callbacks=callbacks)\n",
    "\n",
    "pickle_out = open(summary_path+'History.pickle','wb')\n",
    "pickle.dump(history.history,pickle_out)\n",
    "pickle_out.close()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     ############################# for multiple trail runs\n",
    "#     num_channels = 3\n",
    "#     window_size = 40\n",
    "#     num_vUnits = num_channels*window_size\n",
    "#     num_KCs = 15*num_vUnits\n",
    "#     num_ones = num_KCs*6 #total number of ones in matrix\n",
    "#     p_one = num_ones/(num_vUnits*num_KCs)\n",
    "#     load_path = '/home/jonas/HDD/data/unwindowed/Fourier_WTA_vals_and_inds_15xKCs_RS111_WTA005/'\n",
    "    \n",
    "#     def flatten1(x):\n",
    "#         '''\n",
    "#         flatten tensor x\n",
    "#         -> results in [sample_ch1,sample_ch2,sample_ch3,sample_ch1,sample_ch2,...]\n",
    "#         '''\n",
    "#         flat = tf.reshape(x,[-1])\n",
    "#         return flat\n",
    "\n",
    "#     def WTA_activations_from_values_and_indices_map_fn(values,indices,labels):\n",
    "#         '''\n",
    "#         same as WTA_activations_from_values_and_indices but for a dataset\n",
    "#         '''\n",
    "#         num_KCs = 15*3*40\n",
    "\n",
    "#         num_activations = int(p_WTA*num_KCs)\n",
    "#         batch_size = tf.shape(values)[0]\n",
    "#         indices = tf.stack([tf.stack([tf.range(start=0,limit=batch_size) for i in range(num_activations)],axis=1),\n",
    "#                             indices],axis=-1)\n",
    "#         values = flatten1(values)\n",
    "#         indices = tf.reshape(indices,[-1,2])\n",
    "#         WTA_rec = tf.scatter_nd(indices,values,shape=[batch_size,num_KCs])\n",
    "#         return WTA_rec,labels\n",
    "    \n",
    "#     training_files = []\n",
    "#     validation_files = []\n",
    "#     for file in os.listdir(load_path):\n",
    "#         if 'dataset5' in file:\n",
    "#             validation_files.append(load_path+file)\n",
    "#         elif 'dataset10' in file:\n",
    "#             validation_files.append(load_path+file)\n",
    "#         elif 'dataset15' in file:\n",
    "#             validation_files.append(load_path+file)\n",
    "#         elif 'dataset20' in file:\n",
    "#             validation_files.append(load_path+file)\n",
    "#         elif 'dataset25' in file:\n",
    "#             validation_files.append(load_path+file)\n",
    "#         else:\n",
    "#             training_files.append(load_path+file)\n",
    "#     print(len(training_files))\n",
    "#     print(len(validation_files))\n",
    "#     #############################\n",
    "#############################\n",
    "# dataset = dataset.map(map_func=WTA_activations_from_values_and_indices_map_fn,num_parallel_calls=8)\n",
    "#############################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
